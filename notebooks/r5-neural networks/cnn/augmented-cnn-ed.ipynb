{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Lourde Hajjar\n",
    "\n",
    "This notebook applies a CNN to the edge-detected image dataset, initially testing a basic model without specifying a learning rate. The model was then enhanced by incorporating data augmentation (e.g., rotation, shifting, zooming, and flipping) to improve generalization and batch normalization layers to stabilize training.\n",
    "\n",
    "The modelâ€™s performance was first evaluated using a test set and 10-fold cross-validation, achieving an accuracy of 63%. To further improve performance, a learning rate was added, resulting in an accuracy increase to 66%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN With Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Edge-Detected Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define dataset path\n",
    "data_dir =r\"C:\\Users\\lourd\\OneDrive\\Desktop\\coursework\\datasets\\3_image\\processed\\3_ed_b\"\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# Load images \n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for img_file in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, img_file)\n",
    "            img = load_img(img_path, color_mode=\"grayscale\")\n",
    "            img = load_img(img_path, target_size=(256, 256), color_mode=\"grayscale\")\n",
    "            img_array = img_to_array(img) / 255.0  # Normalize\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_names = ['normal', 'malignant', 'benign']\n",
    "\n",
    "images = images.reshape(-1, 256, 256, 1)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 254, 254, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 125, 125, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 115200)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               29491456  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,585,795\n",
      "Trainable params: 29,585,347\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 38s 400ms/step - loss: 13.2049 - accuracy: 0.4212 - val_loss: 52.7434 - val_accuracy: 0.3531\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 1.1098 - accuracy: 0.4718 - val_loss: 71.5525 - val_accuracy: 0.3531\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 30s 351ms/step - loss: 1.0472 - accuracy: 0.4198 - val_loss: 75.3885 - val_accuracy: 0.3531\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 0.9885 - accuracy: 0.4421 - val_loss: 40.4122 - val_accuracy: 0.3531\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 31s 359ms/step - loss: 1.0391 - accuracy: 0.4421 - val_loss: 9.6734 - val_accuracy: 0.3976\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 29s 341ms/step - loss: 1.0216 - accuracy: 0.4287 - val_loss: 1.9194 - val_accuracy: 0.4510\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 30s 356ms/step - loss: 1.0016 - accuracy: 0.4718 - val_loss: 1.5292 - val_accuracy: 0.4629\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 30s 350ms/step - loss: 1.0926 - accuracy: 0.4770 - val_loss: 5.7771 - val_accuracy: 0.3591\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 30s 354ms/step - loss: 1.1763 - accuracy: 0.4881 - val_loss: 0.9286 - val_accuracy: 0.5905\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 0.9580 - accuracy: 0.5134 - val_loss: 0.9129 - val_accuracy: 0.5608\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 30s 349ms/step - loss: 0.9632 - accuracy: 0.4993 - val_loss: 0.8394 - val_accuracy: 0.6053\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 30s 354ms/step - loss: 0.9434 - accuracy: 0.5163 - val_loss: 0.9506 - val_accuracy: 0.4955\n",
      "Epoch 13/30\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 0.9147 - accuracy: 0.5394 - val_loss: 0.8224 - val_accuracy: 0.5846\n",
      "Epoch 14/30\n",
      "85/85 [==============================] - 30s 356ms/step - loss: 0.8634 - accuracy: 0.5661 - val_loss: 0.9387 - val_accuracy: 0.4926\n",
      "Epoch 15/30\n",
      "85/85 [==============================] - 51s 604ms/step - loss: 0.8452 - accuracy: 0.5773 - val_loss: 0.8314 - val_accuracy: 0.6083\n",
      "Epoch 16/30\n",
      "85/85 [==============================] - 29s 344ms/step - loss: 0.8637 - accuracy: 0.5706 - val_loss: 0.9603 - val_accuracy: 0.4896\n",
      "Epoch 17/30\n",
      "85/85 [==============================] - 31s 364ms/step - loss: 0.8635 - accuracy: 0.5884 - val_loss: 0.7615 - val_accuracy: 0.6588\n",
      "Epoch 18/30\n",
      "85/85 [==============================] - 31s 361ms/step - loss: 0.8441 - accuracy: 0.5817 - val_loss: 1.0012 - val_accuracy: 0.4659\n",
      "Epoch 19/30\n",
      "85/85 [==============================] - 30s 351ms/step - loss: 0.8056 - accuracy: 0.6025 - val_loss: 0.7751 - val_accuracy: 0.6320\n",
      "Epoch 20/30\n",
      "85/85 [==============================] - 30s 356ms/step - loss: 0.8411 - accuracy: 0.5713 - val_loss: 0.7654 - val_accuracy: 0.6439\n",
      "Epoch 21/30\n",
      "85/85 [==============================] - 30s 355ms/step - loss: 0.7920 - accuracy: 0.6122 - val_loss: 0.8865 - val_accuracy: 0.5430\n",
      "Epoch 22/30\n",
      "85/85 [==============================] - 30s 357ms/step - loss: 0.8112 - accuracy: 0.5951 - val_loss: 0.7378 - val_accuracy: 0.6320\n",
      "Epoch 23/30\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 0.7611 - accuracy: 0.6144 - val_loss: 0.8338 - val_accuracy: 0.5579\n",
      "Epoch 24/30\n",
      "85/85 [==============================] - 28s 325ms/step - loss: 0.7885 - accuracy: 0.6070 - val_loss: 0.8712 - val_accuracy: 0.5490\n",
      "Epoch 25/30\n",
      "85/85 [==============================] - 29s 345ms/step - loss: 0.7804 - accuracy: 0.6055 - val_loss: 0.9177 - val_accuracy: 0.5282\n",
      "Epoch 26/30\n",
      "85/85 [==============================] - 31s 369ms/step - loss: 0.7620 - accuracy: 0.6189 - val_loss: 1.1362 - val_accuracy: 0.4036\n",
      "Epoch 27/30\n",
      "85/85 [==============================] - 31s 358ms/step - loss: 0.7635 - accuracy: 0.6189 - val_loss: 1.1795 - val_accuracy: 0.3650\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=16),  \n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.6320474743843079\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 163ms/step\n",
      "Test Accuracy: 0.6320474777448071\n",
      "F1 Score (Macro): 0.5254564705984668\n",
      "F1 Score (Weighted): 0.5526790908625339\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.92      0.66       118\n",
      "           1       0.90      0.87      0.88       119\n",
      "           2       0.17      0.02      0.04       100\n",
      "\n",
      "    accuracy                           0.63       337\n",
      "   macro avg       0.53      0.60      0.53       337\n",
      "weighted avg       0.55      0.63      0.55       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    \n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    \n",
    "    y_true = y_test\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print Results\n",
    "print(\"Test Accuracy:\", np.mean(y_true == y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "\n",
    "# Print Detailed Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print 10-Fold Cross-Validation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 182ms/step\n",
      "5/5 [==============================] - 1s 143ms/step\n",
      "5/5 [==============================] - 1s 143ms/step\n",
      "5/5 [==============================] - 1s 145ms/step\n",
      "5/5 [==============================] - 1s 139ms/step\n",
      "5/5 [==============================] - 1s 130ms/step\n",
      "5/5 [==============================] - 1s 127ms/step\n",
      "5/5 [==============================] - 1s 128ms/step\n",
      "5/5 [==============================] - 1s 127ms/step\n",
      "5/5 [==============================] - 1s 127ms/step\n",
      "10-Fold Cross-Validation Results:\n",
      "Average Accuracy: 0.6166556108347154\n",
      "Average Precision: 0.564604482902382\n",
      "Average Recall: 0.6243303374506269\n",
      "Average F1 Score: 0.5335183382516665\n",
      "Average TP Rate: 0.9319428710550677\n",
      "Average FP Rate: 0.023071854974294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_prob = model.predict(x_val_fold)\n",
    "    y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    precision = precision_score(y_val_fold, y_val_pred, average='macro')\n",
    "    recall = recall_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "\n",
    "\n",
    "    if len(np.unique(y_val_fold)) == 2:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_val_pred_prob[:, 1])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Calculate TP and FP rates from confusion matrix\n",
    "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
    "    tp_rate = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    fp_rate = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
    "\n",
    "    true_positive_rates.append(tp_rate)\n",
    "    false_positive_rates.append(fp_rate)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average of each metric across all folds\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Average TP Rate:\", np.mean(true_positive_rates))\n",
    "print(\"Average FP Rate:\", np.mean(false_positive_rates))\n",
    "\n",
    "if roc_auc_scores:\n",
    "    print(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN With Augmentation and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 254, 254, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 125, 125, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 115200)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               29491456  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,585,795\n",
      "Trainable params: 29,585,347\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an improved CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),  # Batch normalization\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Batch normalization\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Batch normalization\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation for training\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Apply augmentation only to the training set\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 32s 362ms/step - loss: 2.5249 - accuracy: 0.4606 - val_loss: 5.7473 - val_accuracy: 0.2967\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 32s 369ms/step - loss: 1.1436 - accuracy: 0.5163 - val_loss: 6.4885 - val_accuracy: 0.4896\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 0.9740 - accuracy: 0.5104 - val_loss: 8.7301 - val_accuracy: 0.4629\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 31s 364ms/step - loss: 0.8972 - accuracy: 0.5319 - val_loss: 6.9374 - val_accuracy: 0.4451\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 30s 346ms/step - loss: 0.8473 - accuracy: 0.5594 - val_loss: 2.2573 - val_accuracy: 0.4095\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 30s 347ms/step - loss: 0.8008 - accuracy: 0.5840 - val_loss: 1.0079 - val_accuracy: 0.5549\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 30s 349ms/step - loss: 0.8207 - accuracy: 0.5914 - val_loss: 0.6747 - val_accuracy: 0.6677\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 30s 355ms/step - loss: 0.7836 - accuracy: 0.6055 - val_loss: 0.7350 - val_accuracy: 0.6409\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 30s 348ms/step - loss: 0.8013 - accuracy: 0.6062 - val_loss: 0.7307 - val_accuracy: 0.6202\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 29s 346ms/step - loss: 0.7885 - accuracy: 0.6018 - val_loss: 0.9943 - val_accuracy: 0.5252\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 0.7853 - accuracy: 0.6077 - val_loss: 0.7086 - val_accuracy: 0.6350\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 25s 296ms/step - loss: 0.7687 - accuracy: 0.6122 - val_loss: 1.0409 - val_accuracy: 0.4540\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=16),  # Augmented training data\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.6676557660102844\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 125ms/step\n",
      "Test Accuracy: 0.6676557863501483\n",
      "F1 Score (Macro): 0.6511998615657153\n",
      "F1 Score (Weighted): 0.6543899013501677\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.36      0.49       118\n",
      "           1       0.83      0.89      0.86       119\n",
      "           2       0.50      0.76      0.60       100\n",
      "\n",
      "    accuracy                           0.67       337\n",
      "   macro avg       0.69      0.67      0.65       337\n",
      "weighted avg       0.70      0.67      0.65       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "# Get predicted labels (as probabilities)\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Check if y_test is one-hot encoded or not\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    # y_test is one-hot encoded\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    # y_test is not one-hot encoded\n",
    "    y_true = y_test\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print Results\n",
    "print(\"Test Accuracy:\", np.mean(y_true == y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "\n",
    "# Print Detailed Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the 10-Fold Cross-Validation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 104ms/step\n",
      "5/5 [==============================] - 1s 118ms/step\n",
      "5/5 [==============================] - 1s 112ms/step\n",
      "5/5 [==============================] - 1s 114ms/step\n",
      "5/5 [==============================] - 1s 117ms/step\n",
      "5/5 [==============================] - 1s 114ms/step\n",
      "5/5 [==============================] - 1s 121ms/step\n",
      "5/5 [==============================] - 1s 117ms/step\n",
      "5/5 [==============================] - 1s 119ms/step\n",
      "5/5 [==============================] - 1s 118ms/step\n",
      "10-Fold Cross-Validation Results:\n",
      "Average Accuracy: 0.6664842454394693\n",
      "Average Precision: 0.6877979047891465\n",
      "Average Recall: 0.6660144670673734\n",
      "Average F1 Score: 0.6483521089885661\n",
      "Average TP Rate: 0.9447864214185826\n",
      "Average FP Rate: 0.14054147606779183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "   \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_prob = model.predict(x_val_fold)\n",
    "    y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    precision = precision_score(y_val_fold, y_val_pred, average='macro')\n",
    "    recall = recall_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "\n",
    "    # ROC AUC score can only be calculated if there are exactly two classes\n",
    "    if len(np.unique(y_val_fold)) == 2:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_val_pred_prob[:, 1])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Calculate TP and FP rates from confusion matrix\n",
    "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
    "    tp_rate = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    fp_rate = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
    "\n",
    "    true_positive_rates.append(tp_rate)\n",
    "    false_positive_rates.append(fp_rate)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average of each metric across all folds\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Average TP Rate:\", np.mean(true_positive_rates))\n",
    "print(\"Average FP Rate:\", np.mean(false_positive_rates))\n",
    "\n",
    "if roc_auc_scores:\n",
    "    print(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
