{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Author: Lourde Hajjar\n",
    "\n",
    " The notebook applies CNN on the focused image dataset. It builds an improved CNN model.It incorporates data augmentation (e.g., rotation, shifting, zooming, and flipping) and learning rate to enhance generalization and batch normalization layers to stabilize training and accelerate convergence.\n",
    " \n",
    " The modelâ€™s performance is evaluated using a test set and 10-fold cross-validation providing 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN With Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Focused Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goJkhcCqcTdD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define dataset path\n",
    "data_dir =r\"C:\\Users\\lourd\\OneDrive\\Desktop\\coursework\\datasets\\3_image\\processed\\3_foc_b\"\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# Load images \n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for img_file in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, img_file)\n",
    "            img = load_img(img_path, color_mode=\"grayscale\")\n",
    "            img = load_img(img_path, target_size=(256, 256), color_mode=\"grayscale\")\n",
    "            img_array = img_to_array(img) / 255.0  # Normalize\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_names = ['normal', 'malignant', 'benign']\n",
    "\n",
    "\n",
    "images = images.reshape(-1, 256, 256, 1)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "y_F4Ig73dorQ",
    "outputId": "d22e9ad4-d7d2-43c4-8ef7-4a6b8b60bdf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 254, 254, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 125, 125, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 115200)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               29491456  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,585,795\n",
      "Trainable params: 29,585,347\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an improved CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alksM_9Dd3ZS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Apply Data augmentation \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKfvLaG8d6lf",
    "outputId": "963d8706-e7fc-42b1-c99b-adc3ca98a343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 27s 301ms/step - loss: 1.6968 - accuracy: 0.5260 - val_loss: 12.7072 - val_accuracy: 0.3531\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 26s 305ms/step - loss: 0.7721 - accuracy: 0.6226 - val_loss: 22.5731 - val_accuracy: 0.3531\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 28s 330ms/step - loss: 0.6713 - accuracy: 0.6724 - val_loss: 24.6629 - val_accuracy: 0.3531\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 28s 324ms/step - loss: 0.5967 - accuracy: 0.7132 - val_loss: 17.9497 - val_accuracy: 0.3531\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 27s 318ms/step - loss: 0.6149 - accuracy: 0.6828 - val_loss: 7.5537 - val_accuracy: 0.4095\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 26s 310ms/step - loss: 0.5489 - accuracy: 0.7244 - val_loss: 5.3693 - val_accuracy: 0.4273\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 27s 312ms/step - loss: 0.5587 - accuracy: 0.7363 - val_loss: 2.9140 - val_accuracy: 0.4154\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 27s 312ms/step - loss: 0.5116 - accuracy: 0.7318 - val_loss: 0.7979 - val_accuracy: 0.6024\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 26s 311ms/step - loss: 0.5009 - accuracy: 0.7541 - val_loss: 3.9557 - val_accuracy: 0.4006\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 27s 320ms/step - loss: 0.4672 - accuracy: 0.7704 - val_loss: 0.4148 - val_accuracy: 0.8042\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 27s 313ms/step - loss: 0.4845 - accuracy: 0.7637 - val_loss: 0.8274 - val_accuracy: 0.6558\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 27s 314ms/step - loss: 0.5039 - accuracy: 0.7422 - val_loss: 0.5205 - val_accuracy: 0.7537\n",
      "Epoch 13/30\n",
      "85/85 [==============================] - 27s 318ms/step - loss: 0.4743 - accuracy: 0.7600 - val_loss: 1.0946 - val_accuracy: 0.5460\n",
      "Epoch 14/30\n",
      "85/85 [==============================] - 27s 315ms/step - loss: 0.4660 - accuracy: 0.7704 - val_loss: 0.4548 - val_accuracy: 0.7596\n",
      "Epoch 15/30\n",
      "85/85 [==============================] - 26s 311ms/step - loss: 0.4564 - accuracy: 0.7816 - val_loss: 2.0301 - val_accuracy: 0.4748\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=16),  \n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7Wsf724kzYp",
    "outputId": "be394adf-23d8-4ef5-c951-56cbef15c839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8041542768478394\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 6s 190ms/step\n",
      "Test Accuracy: 0.8041543026706232\n",
      "F1 Score (Macro): 0.8002914700162407\n",
      "F1 Score (Weighted): 0.8050834225391164\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.70       118\n",
      "           1       1.00      0.97      0.98       119\n",
      "           2       0.66      0.80      0.72       100\n",
      "\n",
      "    accuracy                           0.80       337\n",
      "   macro avg       0.81      0.80      0.80       337\n",
      "weighted avg       0.81      0.80      0.81       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    \n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    \n",
    "    y_true = y_test\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print Results\n",
    "print(\"Test Accuracy:\", np.mean(y_true == y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print 10-Fold Cross-Validation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 167ms/step\n",
      "5/5 [==============================] - 1s 136ms/step\n",
      "5/5 [==============================] - 1s 159ms/step\n",
      "5/5 [==============================] - 1s 149ms/step\n",
      "5/5 [==============================] - 1s 150ms/step\n",
      "5/5 [==============================] - 1s 146ms/step\n",
      "5/5 [==============================] - 1s 139ms/step\n",
      "5/5 [==============================] - 1s 238ms/step\n",
      "5/5 [==============================] - 1s 219ms/step\n",
      "5/5 [==============================] - 1s 221ms/step\n",
      "10-Fold Cross-Validation Results:\n",
      "Average Accuracy: 0.7905030403537866\n",
      "Average Precision: 0.7995164873463204\n",
      "Average Recall: 0.7907980083113697\n",
      "Average F1 Score: 0.7892427440903667\n",
      "Average TP Rate: 0.9642160982648786\n",
      "Average FP Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "   \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_prob = model.predict(x_val_fold)\n",
    "    y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    precision = precision_score(y_val_fold, y_val_pred, average='macro')\n",
    "    recall = recall_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "\n",
    "   \n",
    "    if len(np.unique(y_val_fold)) == 2:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_val_pred_prob[:, 1])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Calculate TP and FP rates from confusion matrix\n",
    "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
    "    tp_rate = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    fp_rate = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
    "\n",
    "    true_positive_rates.append(tp_rate)\n",
    "    false_positive_rates.append(fp_rate)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average of each metric across all folds\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Average TP Rate:\", np.mean(true_positive_rates))\n",
    "print(\"Average FP Rate:\", np.mean(false_positive_rates))\n",
    "\n",
    "if roc_auc_scores:\n",
    "    print(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
