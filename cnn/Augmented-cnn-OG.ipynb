{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Lourde Hajjar\n",
    "\n",
    "This notebook applies a CNN to the original (raw data) image dataset, initially testing a basic model without specifying a learning rate. The model was then enhanced by incorporating data augmentation (e.g., rotation, shifting, zooming, and flipping) to improve generalization and batch normalization layers to stabilize training.\n",
    "\n",
    "The modelâ€™s performance was first evaluated using a test set and 10-fold cross-validation, achieving an accuracy of 53%. To further improve performance, a learning rate was added, resulting in a significant accuracy increase to 74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN With Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define dataset paths \n",
    "data_dir =r\"C:\\Users\\lourd\\OneDrive\\Desktop\\coursework\\datasets\\3_image\\processed\\3_og_b\"\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# Load images \n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for img_file in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, img_file)\n",
    "            img = load_img(img_path, color_mode=\"grayscale\")\n",
    "            img = load_img(img_path, target_size=(256, 256), color_mode=\"grayscale\")\n",
    "            img_array = img_to_array(img) / 255.0  \n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_names = ['normal', 'malignant', 'benign']\n",
    "\n",
    "\n",
    "images = images.reshape(-1, 256, 256, 1)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 254, 254, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 125, 125, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 115200)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               29491456  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,585,795\n",
      "Trainable params: 29,585,347\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 32s 366ms/step - loss: 5.5743 - accuracy: 0.4079 - val_loss: 104.3528 - val_accuracy: 0.3531\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 30s 353ms/step - loss: 1.0797 - accuracy: 0.4094 - val_loss: 155.4345 - val_accuracy: 0.3531\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 1.0576 - accuracy: 0.4094 - val_loss: 86.4029 - val_accuracy: 0.3531\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 29s 341ms/step - loss: 1.0215 - accuracy: 0.4480 - val_loss: 24.2094 - val_accuracy: 0.3531\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 29s 345ms/step - loss: 0.9919 - accuracy: 0.4473 - val_loss: 4.4259 - val_accuracy: 0.3976\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 30s 353ms/step - loss: 0.9988 - accuracy: 0.4547 - val_loss: 0.9208 - val_accuracy: 0.5015\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 29s 346ms/step - loss: 1.0134 - accuracy: 0.4621 - val_loss: 0.8800 - val_accuracy: 0.5341\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 30s 347ms/step - loss: 0.9744 - accuracy: 0.4829 - val_loss: 1.0237 - val_accuracy: 0.4807\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 29s 338ms/step - loss: 0.9700 - accuracy: 0.5253 - val_loss: 0.9421 - val_accuracy: 0.5312\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 30s 347ms/step - loss: 0.9273 - accuracy: 0.5416 - val_loss: 3.2979 - val_accuracy: 0.4243\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 29s 346ms/step - loss: 0.8841 - accuracy: 0.5691 - val_loss: 1.0203 - val_accuracy: 0.3739\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 29s 340ms/step - loss: 0.8900 - accuracy: 0.5461 - val_loss: 2.8747 - val_accuracy: 0.4006\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=16),  \n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.5341246128082275\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 182ms/step\n",
      "Test Accuracy: 0.5341246290801187\n",
      "F1 Score (Macro): 0.4930347129184017\n",
      "F1 Score (Weighted): 0.49264818677764777\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.11      0.19       118\n",
      "           1       0.83      0.72      0.77       119\n",
      "           2       0.38      0.81      0.52       100\n",
      "\n",
      "    accuracy                           0.53       337\n",
      "   macro avg       0.62      0.55      0.49       337\n",
      "weighted avg       0.63      0.53      0.49       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    \n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    \n",
    "    y_true = y_test\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print Results\n",
    "print(\"Test Accuracy:\", np.mean(y_true == y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "\n",
    "# Print Detailed Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print 10-Fold Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 167ms/step\n",
      "5/5 [==============================] - 1s 168ms/step\n",
      "5/5 [==============================] - 1s 137ms/step\n",
      "5/5 [==============================] - 1s 139ms/step\n",
      "5/5 [==============================] - 1s 128ms/step\n",
      "5/5 [==============================] - 1s 131ms/step\n",
      "5/5 [==============================] - 1s 141ms/step\n",
      "5/5 [==============================] - 1s 137ms/step\n",
      "5/5 [==============================] - 1s 136ms/step\n",
      "5/5 [==============================] - 1s 133ms/step\n",
      "10-Fold Cross-Validation Results:\n",
      "Average Accuracy: 0.5370867882808181\n",
      "Average Precision: 0.60297765643291\n",
      "Average Recall: 0.5337559122127611\n",
      "Average F1 Score: 0.4909344507097996\n",
      "Average TP Rate: 1.0\n",
      "Average FP Rate: 0.24310606060606058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "   \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_prob = model.predict(x_val_fold)\n",
    "    y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    precision = precision_score(y_val_fold, y_val_pred, average='macro')\n",
    "    recall = recall_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "\n",
    "    \n",
    "    if len(np.unique(y_val_fold)) == 2:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_val_pred_prob[:, 1])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Calculate TP and FP rates from confusion matrix\n",
    "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
    "    tp_rate = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    fp_rate = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
    "\n",
    "    true_positive_rates.append(tp_rate)\n",
    "    false_positive_rates.append(fp_rate)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average of each metric across all folds\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Average TP Rate:\", np.mean(true_positive_rates))\n",
    "print(\"Average FP Rate:\", np.mean(false_positive_rates))\n",
    "\n",
    "if roc_auc_scores:\n",
    "    print(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN With Augmentation and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 254, 254, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 127, 127, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 125, 125, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 115200)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               29491456  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,585,795\n",
      "Trainable params: 29,585,347\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Apply Data augmentation \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 28s 315ms/step - loss: 2.2296 - accuracy: 0.4889 - val_loss: 9.0225 - val_accuracy: 0.2967\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 25s 299ms/step - loss: 0.8015 - accuracy: 0.6114 - val_loss: 13.1406 - val_accuracy: 0.3976\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 0.6858 - accuracy: 0.6724 - val_loss: 17.2236 - val_accuracy: 0.4273\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 25s 294ms/step - loss: 0.5728 - accuracy: 0.7043 - val_loss: 11.3045 - val_accuracy: 0.4184\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 64s 762ms/step - loss: 0.5472 - accuracy: 0.7444 - val_loss: 4.7620 - val_accuracy: 0.3561\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 25s 298ms/step - loss: 0.5113 - accuracy: 0.7519 - val_loss: 2.4544 - val_accuracy: 0.4214\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 25s 295ms/step - loss: 0.4818 - accuracy: 0.7689 - val_loss: 3.2147 - val_accuracy: 0.3531\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 25s 297ms/step - loss: 0.4871 - accuracy: 0.7593 - val_loss: 2.4853 - val_accuracy: 0.4095\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 25s 293ms/step - loss: 0.4793 - accuracy: 0.7749 - val_loss: 0.5790 - val_accuracy: 0.7448\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 24s 280ms/step - loss: 0.4786 - accuracy: 0.7689 - val_loss: 1.4920 - val_accuracy: 0.5608\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 27s 312ms/step - loss: 0.4781 - accuracy: 0.7689 - val_loss: 0.8492 - val_accuracy: 0.6024\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 26s 306ms/step - loss: 0.5287 - accuracy: 0.7474 - val_loss: 1.9002 - val_accuracy: 0.4926\n",
      "Epoch 13/30\n",
      "85/85 [==============================] - 26s 302ms/step - loss: 0.5185 - accuracy: 0.7600 - val_loss: 1.3945 - val_accuracy: 0.5163\n",
      "Epoch 14/30\n",
      "85/85 [==============================] - 26s 301ms/step - loss: 0.4735 - accuracy: 0.7741 - val_loss: 1.0418 - val_accuracy: 0.5994\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=16),  \n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.7448071241378784\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 167ms/step\n",
      "Test Accuracy: 0.744807121661721\n",
      "F1 Score (Macro): 0.7330907798788777\n",
      "F1 Score (Weighted): 0.7362411691636712\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.42      0.54       118\n",
      "           1       1.00      0.95      0.97       119\n",
      "           2       0.56      0.88      0.69       100\n",
      "\n",
      "    accuracy                           0.74       337\n",
      "   macro avg       0.77      0.75      0.73       337\n",
      "weighted avg       0.78      0.74      0.74       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make Predictions on the Test Set\n",
    "\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "   \n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    \n",
    "    y_true = y_test\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print Results\n",
    "print(\"Test Accuracy:\", np.mean(y_true == y_pred))\n",
    "print(\"F1 Score (Macro):\", f1_macro)\n",
    "print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "\n",
    "# Print Detailed Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the 10-Fold Cross-Validation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 136ms/step\n",
      "5/5 [==============================] - 1s 134ms/step\n",
      "5/5 [==============================] - 1s 129ms/step\n",
      "5/5 [==============================] - 1s 134ms/step\n",
      "5/5 [==============================] - 1s 143ms/step\n",
      "5/5 [==============================] - 1s 144ms/step\n",
      "5/5 [==============================] - 1s 130ms/step\n",
      "5/5 [==============================] - 1s 139ms/step\n",
      "5/5 [==============================] - 1s 125ms/step\n",
      "5/5 [==============================] - 1s 120ms/step\n",
      "10-Fold Cross-Validation Results:\n",
      "Average Accuracy: 0.709502487562189\n",
      "Average Precision: 0.7241611186070271\n",
      "Average Recall: 0.7078537355792155\n",
      "Average F1 Score: 0.7041295895327543\n",
      "Average TP Rate: 0.9347744783118651\n",
      "Average FP Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "   \n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_val_pred_prob = model.predict(x_val_fold)\n",
    "    y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "    precision = precision_score(y_val_fold, y_val_pred, average='macro')\n",
    "    recall = recall_score(y_val_fold, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='macro')\n",
    "\n",
    "    \n",
    "    if len(np.unique(y_val_fold)) == 2:\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_val_pred_prob[:, 1])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # Calculate TP and FP rates from confusion matrix\n",
    "    cm = confusion_matrix(y_val_fold, y_val_pred)\n",
    "    tp_rate = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "    fp_rate = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) > 0 else 0\n",
    "\n",
    "    true_positive_rates.append(tp_rate)\n",
    "    false_positive_rates.append(fp_rate)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average of each metric across all folds\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "print(\"Average TP Rate:\", np.mean(true_positive_rates))\n",
    "print(\"Average FP Rate:\", np.mean(false_positive_rates))\n",
    "\n",
    "if roc_auc_scores:\n",
    "    print(\"Average ROC AUC Score:\", np.mean(roc_auc_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
